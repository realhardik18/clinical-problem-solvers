- smaller chunks are bad in our use case, cause they dont preserve much context
- ive choosen 200 words per chunk for our use case which seems to be ideal
- for each chunk we will first extract entities using NER(names entity recognition)
- tag each of these entities to their types using UMLS(Unified Medical Language System)
- each chunk has the following data: 
    - text, start time, end time, word count,video_id, chunk index, metadata of the video and medical entities
    - and each given entity has text, cui id and type
    - example of how each chunk will look like 
- now we have around 40k chunks we will generate embeddings for each chunk
- we will generate embeddings using the biobert embeddings model dur to the following reasons
    - it understands medical terms better 
    - it has already been pre trained on a vocabulary of words
    - hence understands clinical context better than anyother given model